{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def call_4o(prompt, image_path=None):\n",
    "    content = [{\"type\": \"text\", \"text\": prompt}]\n",
    "    if image_path:\n",
    "        base64_image = encode_image(image_path)\n",
    "        content.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "            },\n",
    "        })\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def call_4o_text_only(prompt):\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "eval_json_path = \"valid_contents.json\"\n",
    "train_json_path = \"train_contents.json\"\n",
    "\n",
    "with open(eval_json_path, \"r\") as f:\n",
    "    eval_contents = json.load(f)\n",
    "\n",
    "with open(train_json_path, \"r\") as f:\n",
    "    train_contents = json.load(f)\n",
    "\n",
    "valid_dataset = []\n",
    "train_dataset = []\n",
    "\n",
    "for custom_id, content in eval_contents.items():\n",
    "    valid_dataset.append({\n",
    "        \"custom_id\": custom_id,\n",
    "        \"image_path\": f\"data/crowdai/val/images/{custom_id}.jpg\",\n",
    "        \"caption\": content\n",
    "    })\n",
    "\n",
    "for custom_id, content in train_contents.items():\n",
    "    train_dataset.append({\n",
    "        \"custom_id\": custom_id,\n",
    "        \"image_path\": f\"data/crowdai/train/images/{custom_id}.jpg\",\n",
    "        \"caption\": content\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def parse_caption(dataset: List[Dict[str, Any]]):\n",
    "    dirty_dataset = []\n",
    "    good_dataset = []\n",
    "    for item in tqdm(dataset):\n",
    "        caption = item[\"caption\"]\n",
    "        pattern = r\"(\\d+)[:\\s]+([^\\n]+)\"\n",
    "        matches = re.findall(pattern, caption)\n",
    "        descriptions = {int(idx): desc.strip() for idx, desc in matches}\n",
    "        if not descriptions:\n",
    "            dirty_dataset.append(item)\n",
    "            continue\n",
    "        item[\"descriptions\"] = descriptions\n",
    "        good_dataset.append(item)\n",
    "    \n",
    "    return good_dataset, dirty_dataset\n",
    "\n",
    "good_dataset, dirty_dataset = parse_caption(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_dataset[0], len(dirty_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jinja2\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "SYSTEMP_PROMPT = \"\"\"You are a description collator. Please process the input according to the following rules:\n",
    "\n",
    "# Task\n",
    "Extract instance indexes and their corresponding descriptions from the input text.  \n",
    "The instance index is an integer, and it can appear anywhere near the description, possibly surrounded by brackets, parentheses, or separated by spaces, dashes, colons, etc.  \n",
    "You must accurately associate each instance index with its corresponding description, regardless of formatting inconsistencies.\n",
    "\n",
    "# Input\n",
    "{{input_prompt}}\n",
    "\n",
    "# Processing Rules\n",
    "1. For each instance, extract:\n",
    "   - The instance index (an integer).\n",
    "   - The associated description text.\n",
    "2. Rephrase each description into a **noun phrase** that starts with \"**a building**\" or \"**the building**\".\n",
    "   - If the original description already starts with \"a building\" or \"the building\", keep it.\n",
    "   - If it does not, rephrase it naturally so that it does.\n",
    "3. Focus strictly on spatial location, structure, or appearance.  \n",
    "   Do not add new information, actions, or interpretations.\n",
    "4. Ignore all irrelevant symbols, formatting inconsistencies, and line breaks in the input.\n",
    "\n",
    "# Output Format\n",
    "Return the results in **strict JSON format**, where:\n",
    "- Each **key** is the instance index (as a string).\n",
    "- Each **value** is the cleaned and corrected noun phrase.\n",
    "\n",
    "The JSON structure must look like:\n",
    "```json\n",
    "{\n",
    "    \"0\": \"[Noun phrase for instance 0]\",\n",
    "    \"1\": \"[Noun phrase for instance 1]\",\n",
    "    \"2\": \"[Noun phrase for instance 2]\",\n",
    "    ...\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "template = jinja2.Template(SYSTEMP_PROMPT)\n",
    "\n",
    "new_eval_dataset = []\n",
    "\n",
    "for item in tqdm(dirty_dataset):\n",
    "    # image = Image.open(item[\"image_path\"])\n",
    "    caption = item[\"caption\"]\n",
    "    prompt = template.render(input_prompt=caption)\n",
    "\n",
    "    response = call_4o_text_only(prompt)\n",
    "    # Extract the JSON string from the response text\n",
    "    json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "    if json_match:\n",
    "        final_prompt = json.loads(json_match.group())\n",
    "    else:\n",
    "        final_prompt = {}\n",
    "    new_eval_dataset.append({\n",
    "        **item,\n",
    "        \"descriptions\": final_prompt\n",
    "    })\n",
    "    print(final_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
